# HERALD Performance Tuning Guide

## Overview

This guide provides comprehensive performance optimization strategies for HERALD (Hybrid Efficient Reasoning Architecture for Local Deployment), focusing on CPU-optimized inference, memory management, and system-level optimizations.

## Table of Contents

1. [Performance Targets](#performance-targets)
2. [System-Level Optimization](#system-level-optimization)
3. [Application-Level Optimization](#application-level-optimization)
4. [Memory Optimization](#memory-optimization)
5. [CPU Optimization](#cpu-optimization)
6. [Quantization Strategies](#quantization-strategies)
7. [Benchmarking and Monitoring](#benchmarking-and-monitoring)
8. [Troubleshooting Performance Issues](#troubleshooting-performance-issues)

## Performance Targets

### Baseline Performance Metrics

| Metric | Target | Current | Status |
|--------|--------|---------|--------|
| Context Capacity | 1M tokens | 1M tokens | ‚úÖ |
| Peak RAM Usage | <11.8GB | ~8.5GB | ‚úÖ |
| Token Generation | 95/67/43 tokens/sec | 85/60/40 tokens/sec | üîÑ |
| Model Load Time | <700ms | ~5.2s | ‚ùå |
| Memory Efficiency | 8.5:1 compression | 8.2:1 compression | ‚úÖ |

### Performance Categories

- **Short Context** (<1K tokens): 95 tokens/sec target
- **Medium Context** (1K-10K tokens): 67 tokens/sec target  
- **Long Context** (10K-1M tokens): 43 tokens/sec target

## System-Level Optimization

### 1. CPU Governor Configuration

```bash
# Set CPU governor to performance mode
echo 'performance' | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

# Verify settings
cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

# Make permanent (add to /etc/default/cpufrequtils)
GOVERNOR="performance"
```

### 2. Memory Management

```bash
# Optimize swappiness for AI workloads
echo 'vm.swappiness=10' | sudo tee -a /etc/sysctl.conf
echo 'vm.dirty_ratio=15' | sudo tee -a /etc/sysctl.conf
echo 'vm.dirty_background_ratio=5' | sudo tee -a /etc/sysctl.conf

# Apply changes
sudo sysctl -p

# Verify settings
cat /proc/sys/vm/swappiness
cat /proc/sys/vm/dirty_ratio
```

### 3. NUMA Configuration

```bash
# Check NUMA topology
numactl --hardware

# Bind process to specific NUMA node
numactl --cpunodebind=0 --membind=0 python -m api.server

# For multi-socket systems, use interleaved memory
numactl --interleave=all python -m api.server
```

### 4. Disk I/O Optimization

```bash
# For SSD storage, add noatime mount option
echo '/dev/sda1 / ext4 defaults,noatime 0 1' | sudo tee -a /etc/fstab

# Optimize I/O scheduler for SSD
echo 'ACTION=="add|change", KERNEL=="sd[a-z]*", ATTR{queue/scheduler}="none"' | sudo tee /etc/udev/rules.d/60-ioschedulers.rules

# Verify scheduler
cat /sys/block/sda/queue/scheduler
```

### 5. Network Optimization

```bash
# Optimize network buffers
echo 'net.core.rmem_max=16777216' | sudo tee -a /etc/sysctl.conf
echo 'net.core.wmem_max=16777216' | sudo tee -a /etc/sysctl.conf
echo 'net.core.rmem_default=262144' | sudo tee -a /etc/sysctl.conf
echo 'net.core.wmem_default=262144' | sudo tee -a /etc/sysctl.conf

# Apply changes
sudo sysctl -p
```

## Application-Level Optimization

### 1. Model Configuration Optimization

```python
from core.engine import ModelConfig, InferenceConfig

# Optimized model configuration
optimized_config = ModelConfig(
    # Architecture optimization
    num_transformer_layers=12,
    hidden_dim=768,
    num_heads=12,
    head_dim=64,
    
    # Memory optimization
    active_memory_size=8192,
    compressed_memory_size=32768,
    chunk_size=1024,
    chunk_overlap=128,
    
    # Performance targets
    max_context_length=1000000,
    peak_ram_usage=11.8,
    token_generation_speed=0.8,
    model_load_time=0.7,
    
    # Compression settings
    compression_ratio=8.5,
    quantization="int8",
    sparse_matrices=True,
    lz4_compression=True,
    
    # Hardware optimization
    cpu_optimization=True,
    avx512_support=True,
    intel_mkl=True,
    memory_mapping=True,
    simd_vectorization=True,
    bf16_precision=True
)

# Optimized inference configuration
optimized_inference = InferenceConfig(
    max_new_tokens=100,
    temperature=0.7,
    top_p=0.9,
    top_k=50,
    repetition_penalty=1.1,
    
    # Memory management
    enable_attention_caching=True,
    enable_compression=True,
    memory_mapping=True,
    
    # Performance settings
    batch_size=1,
    use_cache=True,
    return_attention_weights=False
)
```

### 2. Engine Initialization Optimization

```python
from core.engine import NeuroEngine
import threading

# Optimize engine initialization
def create_optimized_engine():
    """Create engine with optimized settings."""
    engine = NeuroEngine(optimized_config, optimized_inference)
    
    # Pre-warm the engine
    engine._setup_hardware_optimization()
    
    # Initialize memory manager with optimal settings
    engine._initialize_memory_manager()
    
    # Set up expert router
    engine._setup_expert_router()
    
    return engine

# Thread-safe engine creation
engine_lock = threading.Lock()
_engine_instance = None

def get_engine():
    """Get singleton engine instance."""
    global _engine_instance
    if _engine_instance is None:
        with engine_lock:
            if _engine_instance is None:
                _engine_instance = create_optimized_engine()
    return _engine_instance
```

### 3. Batch Processing Optimization

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class OptimizedBatchProcessor:
    def __init__(self, max_workers=4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.engine = get_engine()
    
    async def process_batch(self, prompts, batch_size=8):
        """Process prompts in optimized batches."""
        results = []
        
        # Split prompts into batches
        for i in range(0, len(prompts), batch_size):
            batch = prompts[i:i + batch_size]
            
            # Process batch concurrently
            batch_results = await asyncio.gather(*[
                self._process_single(prompt) for prompt in batch
            ])
            
            results.extend(batch_results)
        
        return results
    
    async def _process_single(self, prompt):
        """Process single prompt with optimization."""
        loop = asyncio.get_event_loop()
        
        # Run in thread pool to avoid blocking
        return await loop.run_in_executor(
            self.executor,
            self.engine.generate,
            prompt
        )
```

## Memory Optimization

### 1. Multi-Tier Memory Management

```python
from core.memory import MultiTierMemoryManager, MemoryConfig

# Optimized memory configuration
optimized_memory_config = MemoryConfig(
    tier1_capacity=8192,      # Active working memory
    tier2_capacity=32768,     # Compressed summaries
    tier3_capacity=1000000,   # Archived knowledge
    chunk_size=1024,
    chunk_overlap=128,
    compression_ratio=4.0,
    enable_memory_mapping=True,
    enable_compression=True
)

# Initialize optimized memory manager
memory_manager = MultiTierMemoryManager(optimized_memory_config)
```

### 2. Memory Pooling

```python
import numpy as np
from typing import Dict, List

class MemoryPool:
    """Optimized memory pool for tensor operations."""
    
    def __init__(self, initial_size=1000):
        self.pools: Dict[tuple, List[np.ndarray]] = {}
        self.initial_size = initial_size
    
    def get_tensor(self, shape: tuple, dtype=np.float32) -> np.ndarray:
        """Get tensor from pool or create new one."""
        key = (shape, dtype)
        
        if key not in self.pools:
            self.pools[key] = []
        
        if self.pools[key]:
            return self.pools[key].pop()
        else:
            return np.zeros(shape, dtype=dtype)
    
    def return_tensor(self, tensor: np.ndarray):
        """Return tensor to pool."""
        key = (tensor.shape, tensor.dtype)
        
        if key not in self.pools:
            self.pools[key] = []
        
        # Clear tensor data
        tensor.fill(0)
        self.pools[key].append(tensor)
    
    def clear_pools(self):
        """Clear all pools."""
        self.pools.clear()

# Global memory pool
memory_pool = MemoryPool()
```

### 3. Attention Cache Optimization

```python
import time
import numpy as np
from typing import Dict, List

class OptimizedAttentionCache:
    """Optimized attention weight caching."""
    
    def __init__(self, max_cache_size=1000):
        self.cache = {}
        self.max_cache_size = max_cache_size
        self.access_times = {}
    
    def get_cached_attention(self, context_id: str, layer_id: int):
        """Get cached attention weights."""
        key = f"{context_id}_{layer_id}"
        
        if key in self.cache:
            self.access_times[key] = time.time()
            return self.cache[key]
        
        return None
    
    def cache_attention(self, context_id: str, layer_id: int, weights: np.ndarray):
        """Cache attention weights with LRU eviction."""
        key = f"{context_id}_{layer_id}"
        
        # Evict least recently used if cache is full
        if len(self.cache) >= self.max_cache_size:
            oldest_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
            del self.cache[oldest_key]
            del self.access_times[oldest_key]
        
        self.cache[key] = weights
        self.access_times[key] = time.time()
    
    def clear_cache(self):
        """Clear all cached attention weights."""
        self.cache.clear()
        self.access_times.clear()
```

## CPU Optimization

### 1. SIMD Vectorization

```python
import numpy as np
from numba import jit, prange

@jit(nopython=True, parallel=True)
def optimized_matrix_multiply(a, b):
    """Optimized matrix multiplication with SIMD."""
    m, n = a.shape
    n, p = b.shape
    result = np.zeros((m, p), dtype=a.dtype)
    
    for i in prange(m):
        for j in range(p):
            for k in range(n):
                result[i, j] += a[i, k] * b[k, j]
    
    return result

@jit(nopython=True)
def optimized_softmax(x):
    """Optimized softmax implementation."""
    max_x = np.max(x)
    exp_x = np.exp(x - max_x)
    return exp_x / np.sum(exp_x)
```

### 2. Intel MKL Integration

```python
import numpy as np
import os

def setup_mkl_optimization():
    """Setup Intel MKL for optimal performance."""
    try:
        import mkl
        mkl.set_num_threads(os.cpu_count())
        print(f"MKL enabled with {os.cpu_count()} threads")
    except ImportError:
        print("MKL not available, using default BLAS")
    
    # Configure NumPy to use MKL
    np.set_printoptions(precision=8, suppress=True)

def optimized_dot_product(a, b):
    """Optimized dot product using MKL."""
    return np.dot(a, b)

def optimized_matrix_operations():
    """Optimized matrix operations."""
    # Use NumPy's optimized BLAS operations
    a = np.random.rand(1000, 1000)
    b = np.random.rand(1000, 1000)
    
    # These operations will use MKL if available
    c = np.dot(a, b)
    d = np.linalg.inv(a)
    e = np.linalg.eigvals(a)
    
    return c, d, e
```

### 3. Thread Pool Optimization

```python
import threading
from concurrent.futures import ThreadPoolExecutor
import queue

class OptimizedThreadPool:
    """Optimized thread pool for CPU-intensive tasks."""
    
    def __init__(self, max_workers=None):
        self.max_workers = max_workers or min(32, os.cpu_count() + 4)
        self.executor = ThreadPoolExecutor(max_workers=self.max_workers)
        self.task_queue = queue.Queue()
        self.result_queue = queue.Queue()
    
    def submit_task(self, func, *args, **kwargs):
        """Submit task to thread pool."""
        future = self.executor.submit(func, *args, **kwargs)
        return future
    
    def process_batch(self, tasks):
        """Process batch of tasks efficiently."""
        futures = []
        
        for task in tasks:
            future = self.submit_task(task['func'], *task['args'], **task['kwargs'])
            futures.append(future)
        
        # Collect results
        results = []
        for future in futures:
            try:
                result = future.result(timeout=300)  # 5 minute timeout
                results.append(result)
            except Exception as e:
                results.append({'error': str(e)})
        
        return results
    
    def shutdown(self):
        """Shutdown thread pool."""
        self.executor.shutdown(wait=True)
```

## Quantization Strategies

### 1. Dynamic Quantization

```python
import torch
import numpy as np

class DynamicQuantizer:
    """Dynamic quantization for model weights."""
    
    def __init__(self, dtype='int8'):
        self.dtype = dtype
        self.scale_factors = {}
    
    def quantize_tensor(self, tensor: np.ndarray, name: str) -> tuple:
        """Quantize tensor to specified precision."""
        if self.dtype == 'int8':
            return self._quantize_int8(tensor, name)
        elif self.dtype == 'bf16':
            return self._quantize_bf16(tensor, name)
        else:
            return tensor, None
    
    def _quantize_int8(self, tensor: np.ndarray, name: str) -> tuple:
        """Quantize to int8."""
        # Calculate scale factor
        max_val = np.max(np.abs(tensor))
        scale = 127.0 / max_val if max_val > 0 else 1.0
        
        # Quantize
        quantized = np.round(tensor * scale).astype(np.int8)
        
        # Store scale factor for dequantization
        self.scale_factors[name] = scale
        
        return quantized, scale
    
    def _quantize_bf16(self, tensor: np.ndarray, name: str) -> tuple:
        """Quantize to bfloat16."""
        # Convert to bfloat16 (simplified)
        # In practice, use proper bfloat16 conversion
        return tensor.astype(np.float32), None
    
    def dequantize_tensor(self, quantized: np.ndarray, name: str) -> np.ndarray:
        """Dequantize tensor."""
        if name in self.scale_factors:
            scale = self.scale_factors[name]
            return quantized.astype(np.float32) / scale
        else:
            return quantized.astype(np.float32)

# Global quantizer
quantizer = DynamicQuantizer(dtype='int8')
```

### 2. Mixed Precision Training

```python
class MixedPrecisionEngine:
    """Mixed precision inference engine."""
    
    def __init__(self, model_config):
        self.model_config = model_config
        self.quantizer = DynamicQuantizer()
        self.fp16_ops = ['attention', 'ffn']
        self.int8_ops = ['embedding', 'output']
    
    def optimize_layer(self, layer_name: str, weights: np.ndarray) -> np.ndarray:
        """Optimize layer with appropriate precision."""
        if any(op in layer_name for op in self.fp16_ops):
            return weights.astype(np.float16)
        elif any(op in layer_name for op in self.int8_ops):
            quantized, _ = self.quantizer.quantize_tensor(weights, layer_name)
            return quantized
        else:
            return weights
    
    def apply_mixed_precision(self, model_weights: dict) -> dict:
        """Apply mixed precision to model weights."""
        optimized_weights = {}
        
        for name, weights in model_weights.items():
            optimized_weights[name] = self.optimize_layer(name, weights)
        
        return optimized_weights
```

### 3. Sparse Matrix Optimization

```python
from scipy import sparse

class SparseMatrixOptimizer:
    """Optimize sparse matrix operations."""
    
    def __init__(self, sparsity_threshold=0.1):
        self.sparsity_threshold = sparsity_threshold
    
    def should_sparsify(self, matrix: np.ndarray) -> bool:
        """Determine if matrix should be sparsified."""
        sparsity = 1.0 - np.count_nonzero(matrix) / matrix.size
        return sparsity > self.sparsity_threshold
    
    def sparsify_matrix(self, matrix: np.ndarray) -> sparse.csr_matrix:
        """Convert dense matrix to sparse format."""
        return sparse.csr_matrix(matrix)
    
    def optimize_weights(self, weights: dict) -> dict:
        """Optimize model weights with sparsification."""
        optimized = {}
        
        for name, weight in weights.items():
            if self.should_sparsify(weight):
                optimized[name] = self.sparsify_matrix(weight)
            else:
                optimized[name] = weight
        
        return optimized
```

## Benchmarking and Monitoring

### 1. Performance Benchmarking

```python
import time
import psutil
import threading
from typing import Dict, List

class PerformanceBenchmark:
    """Comprehensive performance benchmarking."""
    
    def __init__(self):
        self.metrics = {}
        self.monitoring_thread = None
        self.should_monitor = False
    
    def benchmark_inference(self, engine, prompts: List[str], num_runs: int = 5) -> Dict:
        """Benchmark inference performance."""
        results = {
            'total_time': 0,
            'avg_time_per_prompt': 0,
            'tokens_per_second': 0,
            'memory_usage': [],
            'cpu_usage': []
        }
        
        # Start monitoring
        self.start_monitoring()
        
        try:
            for run in range(num_runs):
                run_start = time.time()
                
                for prompt in prompts:
                    start_time = time.time()
                    output = engine.generate(prompt)
                    end_time = time.time()
                    
                    results['total_time'] += end_time - start_time
                
                run_end = time.time()
                print(f"Run {run + 1}: {run_end - run_start:.2f}s")
        
        finally:
            self.stop_monitoring()
        
        # Calculate metrics
        total_prompts = len(prompts) * num_runs
        results['avg_time_per_prompt'] = results['total_time'] / total_prompts
        results['tokens_per_second'] = total_prompts / results['total_time']
        results['avg_memory_usage'] = np.mean(results['memory_usage'])
        results['avg_cpu_usage'] = np.mean(results['cpu_usage'])
        
        return results
    
    def start_monitoring(self):
        """Start resource monitoring."""
        self.should_monitor = True
        self.monitoring_thread = threading.Thread(target=self._monitor_resources)
        self.monitoring_thread.start()
    
    def stop_monitoring(self):
        """Stop resource monitoring."""
        self.should_monitor = False
        if self.monitoring_thread:
            self.monitoring_thread.join()
    
    def _monitor_resources(self):
        """Monitor system resources."""
        while self.should_monitor:
            # Memory usage
            memory = psutil.virtual_memory()
            self.metrics.setdefault('memory_usage', []).append(memory.percent)
            
            # CPU usage
            cpu = psutil.cpu_percent(interval=1)
            self.metrics.setdefault('cpu_usage', []).append(cpu)
            
            time.sleep(1)
    
    def generate_report(self) -> str:
        """Generate performance report."""
        report = []
        report.append("=== HERALD Performance Report ===")
        report.append(f"Average Memory Usage: {np.mean(self.metrics.get('memory_usage', [0])):.1f}%")
        report.append(f"Average CPU Usage: {np.mean(self.metrics.get('cpu_usage', [0])):.1f}%")
        report.append(f"Peak Memory Usage: {np.max(self.metrics.get('memory_usage', [0])):.1f}%")
        report.append(f"Peak CPU Usage: {np.max(self.metrics.get('cpu_usage', [0])):.1f}%")
        
        return "\n".join(report)

# Global benchmark instance
benchmark = PerformanceBenchmark()
```

### 2. Real-time Monitoring

```python
import asyncio
import aiohttp
from datetime import datetime

class RealTimeMonitor:
    """Real-time performance monitoring."""
    
    def __init__(self, api_url="http://localhost:8000"):
        self.api_url = api_url
        self.metrics_history = []
    
    async def monitor_endpoint(self, endpoint: str):
        """Monitor specific API endpoint."""
        async with aiohttp.ClientSession() as session:
            start_time = time.time()
            
            try:
                async with session.get(f"{self.api_url}/{endpoint}") as response:
                    end_time = time.time()
                    response_time = end_time - start_time
                    
                    metric = {
                        'timestamp': datetime.now(),
                        'endpoint': endpoint,
                        'response_time': response_time,
                        'status_code': response.status,
                        'success': response.status == 200
                    }
                    
                    self.metrics_history.append(metric)
                    
                    return metric
            
            except Exception as e:
                return {
                    'timestamp': datetime.now(),
                    'endpoint': endpoint,
                    'response_time': None,
                    'status_code': None,
                    'success': False,
                    'error': str(e)
                }
    
    async def continuous_monitoring(self, interval: int = 30):
        """Continuously monitor performance."""
        while True:
            # Monitor health endpoint
            health_metric = await self.monitor_endpoint("health")
            
            # Monitor stats endpoint
            stats_metric = await self.monitor_endpoint("stats")
            
            # Log metrics
            print(f"Health: {health_metric['response_time']:.3f}s")
            print(f"Stats: {stats_metric['response_time']:.3f}s")
            
            await asyncio.sleep(interval)
    
    def get_performance_summary(self) -> Dict:
        """Get performance summary."""
        if not self.metrics_history:
            return {}
        
        successful_requests = [m for m in self.metrics_history if m['success']]
        
        if not successful_requests:
            return {'error': 'No successful requests'}
        
        response_times = [m['response_time'] for m in successful_requests]
        
        return {
            'total_requests': len(self.metrics_history),
            'successful_requests': len(successful_requests),
            'success_rate': len(successful_requests) / len(self.metrics_history),
            'avg_response_time': np.mean(response_times),
            'min_response_time': np.min(response_times),
            'max_response_time': np.max(response_times),
            'p95_response_time': np.percentile(response_times, 95),
            'p99_response_time': np.percentile(response_times, 99)
        }
```

## Troubleshooting Performance Issues

### 1. Memory Leaks

```python
import gc
import tracemalloc

class MemoryLeakDetector:
    """Detect and fix memory leaks."""
    
    def __init__(self):
        self.snapshots = []
        tracemalloc.start()
    
    def take_snapshot(self, name: str):
        """Take memory snapshot."""
        snapshot = tracemalloc.take_snapshot()
        self.snapshots.append((name, snapshot))
        
        # Print current memory usage
        current, peak = tracemalloc.get_traced_memory()
        print(f"{name}: Current: {current / 1024 / 1024:.1f}MB, Peak: {peak / 1024 / 1024:.1f}MB")
    
    def compare_snapshots(self, snapshot1_name: str, snapshot2_name: str):
        """Compare two snapshots."""
        snapshot1 = None
        snapshot2 = None
        
        for name, snapshot in self.snapshots:
            if name == snapshot1_name:
                snapshot1 = snapshot
            elif name == snapshot2_name:
                snapshot2 = snapshot
        
        if snapshot1 and snapshot2:
            top_stats = snapshot2.compare_to(snapshot1, 'lineno')
            print(f"Top 10 differences between {snapshot1_name} and {snapshot2_name}:")
            for stat in top_stats[:10]:
                print(stat)
    
    def force_garbage_collection(self):
        """Force garbage collection."""
        collected = gc.collect()
        print(f"Garbage collected: {collected} objects")
    
    def get_memory_usage(self) -> Dict:
        """Get detailed memory usage."""
        current, peak = tracemalloc.get_traced_memory()
        
        return {
            'current_mb': current / 1024 / 1024,
            'peak_mb': peak / 1024 / 1024,
            'snapshots_count': len(self.snapshots)
        }

# Global memory detector
memory_detector = MemoryLeakDetector()
```

### 2. CPU Bottlenecks

```python
import cProfile
import pstats
import io

class CPUBottleneckDetector:
    """Detect CPU bottlenecks."""
    
    def __init__(self):
        self.profiler = cProfile.Profile()
        self.stats = None
    
    def profile_function(self, func, *args, **kwargs):
        """Profile a function."""
        self.profiler.enable()
        result = func(*args, **kwargs)
        self.profiler.disable()
        
        # Get stats
        s = io.StringIO()
        ps = pstats.Stats(self.profiler, stream=s).sort_stats('cumulative')
        ps.print_stats(20)
        
        self.stats = s.getvalue()
        return result
    
    def get_profile_summary(self) -> str:
        """Get profiling summary."""
        if self.stats:
            return self.stats
        else:
            return "No profiling data available"
    
    def analyze_bottlenecks(self) -> List[str]:
        """Analyze CPU bottlenecks."""
        bottlenecks = []
        
        if self.stats:
            lines = self.stats.split('\n')
            for line in lines[3:13]:  # Skip header
                if line.strip():
                    parts = line.split()
                    if len(parts) >= 6:
                        function = parts[5]
                        cumulative_time = float(parts[0])
                        
                        if cumulative_time > 1.0:  # More than 1 second
                            bottlenecks.append(f"{function}: {cumulative_time:.2f}s")
        
        return bottlenecks

# Global CPU detector
cpu_detector = CPUBottleneckDetector()
```

### 3. Performance Optimization Checklist

```python
class PerformanceOptimizer:
    """Comprehensive performance optimization."""
    
    def __init__(self):
        self.optimizations_applied = []
    
    def run_optimization_checklist(self, engine) -> Dict:
        """Run complete optimization checklist."""
        results = {
            'memory_optimized': False,
            'cpu_optimized': False,
            'quantization_applied': False,
            'caching_enabled': False,
            'threading_optimized': False
        }
        
        # Memory optimization
        try:
            engine.optimize_memory()
            results['memory_optimized'] = True
            self.optimizations_applied.append('Memory optimization applied')
        except Exception as e:
            print(f"Memory optimization failed: {e}")
        
        # CPU optimization
        try:
            engine._setup_hardware_optimization()
            results['cpu_optimized'] = True
            self.optimizations_applied.append('CPU optimization applied')
        except Exception as e:
            print(f"CPU optimization failed: {e}")
        
        # Quantization
        try:
            if hasattr(engine, 'apply_quantization'):
                engine.apply_quantization()
                results['quantization_applied'] = True
                self.optimizations_applied.append('Quantization applied')
        except Exception as e:
            print(f"Quantization failed: {e}")
        
        # Caching
        try:
            if hasattr(engine, 'enable_caching'):
                engine.enable_caching()
                results['caching_enabled'] = True
                self.optimizations_applied.append('Caching enabled')
        except Exception as e:
            print(f"Caching failed: {e}")
        
        # Threading
        try:
            if hasattr(engine, 'optimize_threading'):
                engine.optimize_threading()
                results['threading_optimized'] = True
                self.optimizations_applied.append('Threading optimized')
        except Exception as e:
            print(f"Threading optimization failed: {e}")
        
        return results
    
    def get_optimization_report(self) -> str:
        """Get optimization report."""
        report = []
        report.append("=== Performance Optimization Report ===")
        
        for optimization in self.optimizations_applied:
            report.append(f"‚úÖ {optimization}")
        
        if not self.optimizations_applied:
            report.append("‚ùå No optimizations applied")
        
        return "\n".join(report)

# Global optimizer
optimizer = PerformanceOptimizer()
```

### 4. Performance Testing Script

```python
#!/usr/bin/env python3
"""
HERALD Performance Testing Script
Usage: python performance_test.py
"""

import asyncio
import time
from core.engine import NeuroEngine, ModelConfig, InferenceConfig

async def run_performance_tests():
    """Run comprehensive performance tests."""
    print("=== HERALD Performance Testing ===")
    
    # Initialize engine
    print("Initializing engine...")
    engine = NeuroEngine()
    
    # Run optimization checklist
    print("Running optimization checklist...")
    results = optimizer.run_optimization_checklist(engine)
    print(optimizer.get_optimization_report())
    
    # Benchmark inference
    print("Benchmarking inference...")
    test_prompts = [
        "Hello, how are you?",
        "Explain quantum computing in simple terms.",
        "Write a short story about a robot learning to paint.",
        "What are the benefits of renewable energy?",
        "How does machine learning work?"
    ]
    
    benchmark_results = benchmark.benchmark_inference(engine, test_prompts, num_runs=3)
    
    print("=== Benchmark Results ===")
    print(f"Average time per prompt: {benchmark_results['avg_time_per_prompt']:.3f}s")
    print(f"Tokens per second: {benchmark_results['tokens_per_second']:.2f}")
    print(f"Average memory usage: {benchmark_results['avg_memory_usage']:.1f}%")
    print(f"Average CPU usage: {benchmark_results['avg_cpu_usage']:.1f}%")
    
    # Memory leak detection
    print("Checking for memory leaks...")
    memory_detector.take_snapshot("start")
    
    # Run some operations
    for i in range(10):
        engine.generate("Test prompt for memory leak detection.")
    
    memory_detector.take_snapshot("end")
    memory_detector.compare_snapshots("start", "end")
    
    # CPU profiling
    print("Profiling CPU usage...")
    profiled_result = cpu_detector.profile_function(
        engine.generate, "This is a test prompt for CPU profiling."
    )
    
    bottlenecks = cpu_detector.analyze_bottlenecks()
    if bottlenecks:
        print("CPU Bottlenecks detected:")
        for bottleneck in bottlenecks:
            print(f"  - {bottleneck}")
    
    print("Performance testing completed!")

if __name__ == "__main__":
    asyncio.run(run_performance_tests())
```

This comprehensive performance tuning guide provides detailed strategies for optimizing HERALD's performance across all aspects of the system, from low-level hardware optimizations to high-level application tuning. 